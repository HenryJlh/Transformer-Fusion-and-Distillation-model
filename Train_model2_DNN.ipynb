{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn,optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "import torchvision.models as models\n",
    "from torch.optim import SGD\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# parameters\n",
    "torch.manual_seed(1)\n",
    "batch_size = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read data from images and excel-labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the images and names\n",
    "IMAGE_DIR =  'Data/C_img_jpg'\n",
    "def read_images(image_path=IMAGE_DIR):\n",
    "    images = []\n",
    "    images_names = [image for image in os.listdir(image_path) if not image.startswith('.')] \n",
    "    for image_name in images_names: \n",
    "            img = Image.open (os.path.join(image_path, image_name))\n",
    "            images.append(img)\n",
    "    return images,images_names\n",
    "images,names = read_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the label\n",
    "excel=pd.read_csv('Data/C_all_data_add_20211007_xz.csv', encoding = 'gb2312')\n",
    "excel = np.array(excel)\n",
    "excel = excel.tolist()\n",
    "# Alligning the labels with images. (i.e. the first label corresponds to the first label.)\n",
    "labels = []\n",
    "sub_labels = []\n",
    "\n",
    "for i in range(len(names)):\n",
    "    for j in range(len(excel)):\n",
    "        if names[i] == excel[j][0]:\n",
    "            labels.append(excel[j][1])\n",
    "            sub_labels.append(excel[j][2:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the training and testing dataet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified Random Sampling\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "split = StratifiedShuffleSplit(n_splits = 1,test_size = 0.2,random_state = 42)\n",
    "\n",
    "names = np.array(names)\n",
    "labels = np.array(labels)\n",
    "names = names.reshape((names.shape[0],-1))\n",
    "labels = labels.reshape((labels.shape[0],-1))\n",
    "data = np.hstack((names,labels)) # hstack:each name corresponds to one label \n",
    "for train_index,test_index in split.split(data,data[:,-1]):\n",
    "    train_set = data[train_index,:]\n",
    "    test_set = data[test_index,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader\n",
    "train_data_2 = train_set\n",
    "test_data_2 = test_set\n",
    "train_loader_2=DataLoader(train_data_2 , batch_size=batch_size ,shuffle=True)\n",
    "test_loader_2=DataLoader(test_data_2, batch_size=batch_size,shuffle=False )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model-2 : DNN\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes=7):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self. fc = nn.Linear(hidden_size, 7)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out= self.fc1(x)\n",
    "        out= self.relu(out)\n",
    "        out= self.dropout(out)\n",
    "        out= self.fc2(out)\n",
    "        out= self.relu(out)\n",
    "        out= self.fc(out)\n",
    "        return out\n",
    "\n",
    "device=torch.device('cuda')\n",
    "model_2=Net(6, 384, 7)\n",
    "model_2=model_2.to(device)\n",
    "print(model_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 2 training\n",
    "learning_rate = 0.001\n",
    "num_epoches = 20\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "optimizer=SGD(model_2.parameters(),lr=learning_rate)\n",
    "\n",
    "\n",
    "train_loss_all = []   # store the loss of training set\n",
    "train_accur_all = []  # store the accuracy of training set\n",
    "test_loss_all = []    # store the loss of test set\n",
    "test_accur_all = []   # store the accuracy of test set\n",
    "\n",
    "# start training\n",
    "for epoch in range(num_epoches):\n",
    "    running_loss=0.0\n",
    "    running_acc=0.0\n",
    "    model_2.train()\n",
    "    for i,data in enumerate(train_loader_2,1):\n",
    "        inf=data[:,0:6].float()\n",
    "        label=data[:,6]\n",
    "        inf=Variable(inf)\n",
    "        label=Variable(label)\n",
    "        inf=inf.cuda()\n",
    "        label=label.cuda()\n",
    "        # forward\n",
    "        out=model_2(inf)\n",
    "        loss=criterion(out,label)\n",
    "        running_loss+=loss.data*label.size(0)\n",
    "        _,pred=torch.max(out,1)\n",
    "        num_correct=(pred==label).sum()\n",
    "        running_acc+=num_correct.data\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print('Train {} epoch, Loss: {:.6f},Acc: {:.6f}'.format(epoch+1,running_loss/(len(train_data_2)),running_acc/len(train_data_2)))\n",
    "    train_loss_all.append(running_loss / len(train_data_2))   # store the loss of training set, and then plot it\n",
    "    train_accur_all.append(running_acc/len(train_data_2))     # store the accuracy of training set, and then plot it\n",
    "    \n",
    "    model_2.eval()\n",
    "    with torch.no_grad():\n",
    "        eval_loss = 0\n",
    "        eval_acc = 0\n",
    "        train_loss_results = []\n",
    "\n",
    "        for data in test_loader_2:\n",
    "            inf=data[:,0:6].float()\n",
    "            label=data[:,6]\n",
    "            inf = Variable(inf)\n",
    "            inf=inf.cuda()\n",
    "            label=label.cuda()\n",
    "            out = model_2(inf)\n",
    "            loss = criterion(out, label)\n",
    "            eval_loss += loss.data\n",
    "            _,pred = torch.max(out,1)\n",
    "            num_correct = (pred == label).sum()\n",
    "            eval_acc += num_correct.data\n",
    "\n",
    "    print('Test Loss: {:,.6f}, Acc: {:,.6f}'.format(eval_loss/(len(test_data_2)), eval_acc/(len(test_data_2))))\n",
    "    test_loss_all.append(eval_loss/(len(test_data_2)))\n",
    "    test_accur_all.append(eval_acc/(len(test_data_2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer the data to cpu\n",
    "for i in range(len(train_loss_all)):\n",
    "    train_loss_all[i] = torch.Tensor.cpu(train_loss_all[i])\n",
    "    train_accur_all[i] = torch.Tensor.cpu(train_accur_all[i])\n",
    "    test_loss_all[i] = torch.Tensor.cpu(test_loss_all[i])\n",
    "    test_accur_all[i] = torch.Tensor.cpu(test_accur_all[i])\n",
    "\n",
    "epoches = [i+1 for i in range(num_epoches)]\n",
    "\n",
    "# plot the loss and accuracy\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(epoches, train_loss_all,\"ro-\", label=\"Train loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(epoches, train_accur_all,\"ro-\", label=\"Train accur\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"acc\")\n",
    "plt.legend()\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(epoches, test_loss_all,\"bo-\", label=\"Test loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(epoches, test_accur_all,\"bo-\", label=\"Test accur\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"acc\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_2, '/Mmodel_2_new.pth')\n",
    "torch.save(model_2.state_dict(),'model_2_param_new.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
