{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn,optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "import torchvision.models as models\n",
    "from torch.optim import SGD\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cross_decomposition import CCA\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# parameters\n",
    "torch.manual_seed(1)\n",
    "batch_size = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read data from images and excel-labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the images and names\n",
    "IMAGE_DIR =  'Data/C_img_jpg'\n",
    "def read_images(image_path=IMAGE_DIR):\n",
    "    images = []\n",
    "    images_names = [image for image in os.listdir(image_path) if not image.startswith('.')] \n",
    "    for image_name in images_names: \n",
    "            img = Image.open (os.path.join(image_path, image_name))\n",
    "            images.append(img)\n",
    "    return images,images_names\n",
    "images,names = read_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the label\n",
    "excel=pd.read_csv('Data/C_all_data_add_20211007_xz.csv', encoding = 'gb2312')\n",
    "excel = np.array(excel)\n",
    "excel = excel.tolist()\n",
    "# Alligning the labels with images. (i.e. the first label corresponds to the first label.)\n",
    "labels = []\n",
    "sub_labels = []\n",
    "\n",
    "for i in range(len(names)):\n",
    "    for j in range(len(excel)):\n",
    "        if names[i] == excel[j][0]:\n",
    "            labels.append(excel[j][1])\n",
    "            sub_labels.append(excel[j][2:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the training and testing dataet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified Random Sampling\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "split = StratifiedShuffleSplit(n_splits = 1,test_size = 0.2,random_state = 42)\n",
    "\n",
    "names = np.array(names)\n",
    "labels = np.array(labels)\n",
    "names = names.reshape((names.shape[0],-1))\n",
    "labels = labels.reshape((labels.shape[0],-1))\n",
    "data = np.hstack((names,labels)) # hstack:each name corresponds to one label \n",
    "for train_index,test_index in split.split(data,data[:,-1]):\n",
    "    train_set = data[train_index,:]\n",
    "    test_set = data[test_index,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data trsnformation\n",
    "train_transforms = transforms.Compose([transforms.RandomHorizontalFlip(0.5),\n",
    "                                       transforms.Resize(256),\n",
    "                                       transforms.CenterCrop(224),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       \n",
    "                                       ])\n",
    "train_data_1 = []\n",
    "train_data_3 = []\n",
    "for row in range(len(train_set)):\n",
    "    for i in range(len(names)):\n",
    "        if names[i] == train_set[row][0]: \n",
    "            img_data = train_transforms(images[i]) \n",
    "            # train_set is [names, labels], we need to get the train_data_1 [images, labels], train_data_3 [images, sub_labels, labels]\n",
    "            train_data_1.append((img_data,int(train_set[row][1]))) \n",
    "            train_data_3.append((img_data,torch.Tensor(sub_labels[i][:]),int(train_set[row][1])))\n",
    "\n",
    "test_transforms =  transforms.Compose([transforms.Resize(256),\n",
    "                                       transforms.CenterCrop(224),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       \n",
    "                                       ])\n",
    "test_data_1 = []\n",
    "test_data_3 = []\n",
    "for row in range(len(test_set)):\n",
    "    for i in range(len(names)):\n",
    "        if names[i] == test_set[row][0]:\n",
    "            img_data = test_transforms(images[i])\n",
    "            test_data_1.append((img_data,int(test_set[row][1]))) \n",
    "            test_data_3.append((img_data,torch.Tensor(sub_labels[i][:]),int(train_set[row][1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader\n",
    "train_loader_3=DataLoader(train_data_3 , batch_size=batch_size ,shuffle=True)\n",
    "test_loader_3=DataLoader(test_data_3, batch_size=int(batch_size*test_size/(1-test_size)),shuffle=False )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model-2\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes=7):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self. fc = nn.Linear(hidden_size, 7)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out= self.fc1(x)\n",
    "        out= self.relu(out)\n",
    "        out= self.dropout(out)\n",
    "        out= self.fc2(out)\n",
    "        out= self.relu(out)\n",
    "        out= self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "original code from rwightman:\n",
    "https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n",
    "\"\"\"\n",
    "from functools import partial\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
    "    \"\"\"\n",
    "    Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n",
    "    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n",
    "    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n",
    "    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n",
    "    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n",
    "    'survival rate' as the argument.\n",
    "    \"\"\"\n",
    "    if drop_prob == 0. or not training:\n",
    "        return x\n",
    "    keep_prob = 1 - drop_prob\n",
    "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
    "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "    random_tensor.floor_()  # binarize\n",
    "    output = x.div(keep_prob) * random_tensor\n",
    "    return output\n",
    "\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"\n",
    "    Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
    "    \"\"\"\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training)\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\"\n",
    "    2D Image to Patch Embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim=128, norm_layer=None):\n",
    "        super().__init__()\n",
    "        '''img_size = (img_size, img_size)\n",
    "        patch_size = (patch_size, patch_size)\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.grid_size = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])\n",
    "        self.num_patches = self.grid_size[0] * self.grid_size[1]\n",
    "\n",
    "        self.proj = nn.Conv2d(in_c, embed_dim, kernel_size=patch_size, stride=patch_size)'''\n",
    "        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''B, C, H, W = x.shape\n",
    "        assert H == self.img_size[0] and W == self.img_size[1], \\\n",
    "            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "\n",
    "        # flatten: [B, C, H, W] -> [B, C, HW]\n",
    "        # transpose: [B, C, HW] -> [B, HW, C'''\n",
    "        #x = x.transpose(1, 2)  #[B,2,128]\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self,\n",
    "                 dim,   # 输入token的dim\n",
    "                 num_heads=8,\n",
    "                 qkv_bias=False,\n",
    "                 qk_scale=None,\n",
    "                 attn_drop_ratio=0.1,\n",
    "                 proj_drop_ratio=0.1):\n",
    "        super(Attention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop_ratio)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop_ratio)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # [batch_size, num_patches + 1, total_embed_dim] [B, 3, 128]\n",
    "        B, N, C = x.shape\n",
    "\n",
    "        # qkv(): -> [batch_size, num_patches + 1, 3 * total_embed_dim][B, 3, 3*128]\n",
    "        # reshape: -> [batch_size, num_patches + 1, 3, num_heads, embed_dim_per_head]\n",
    "        # permute: -> [3, batch_size, num_heads /8, num_patches + 1 /3, embed_dim_per_head /16]\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        # [batch_size, num_heads, num_patches + 1, embed_dim_per_head] [B,8,3,16]\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        # transpose: -> [batch_size, num_heads, embed_dim_per_head, num_patches + 1]\n",
    "        # @: multiply -> [batch_size, num_heads, num_patches + 1, num_patches + 1] [B,8,3,3]\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        # @: multiply -> [batch_size, num_heads, num_patches + 1, embed_dim_per_head]\n",
    "        # transpose: -> [batch_size, num_patches + 1, num_heads, embed_dim_per_head]\n",
    "        # reshape: -> [batch_size, num_patches + 1, total_embed_dim]\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    \"\"\"\n",
    "    MLP as used in Vision Transformer, MLP-Mixer and related networks\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self,\n",
    "                 dim,\n",
    "                 num_heads,\n",
    "                 mlp_ratio=4.,\n",
    "                 qkv_bias=False,\n",
    "                 qk_scale=None,\n",
    "                 drop_ratio=0.,\n",
    "                 attn_drop_ratio=0.,\n",
    "                 drop_path_ratio=0.,\n",
    "                 act_layer=nn.GELU,\n",
    "                 norm_layer=nn.LayerNorm):\n",
    "        super(Block, self).__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                              attn_drop_ratio=attn_drop_ratio, proj_drop_ratio=drop_ratio)\n",
    "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
    "        self.drop_path = DropPath(drop_path_ratio) if drop_path_ratio > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop_ratio)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.drop_path(self.attn(self.norm1(x)))\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, num_classes=7, embed_dim=128, depth=12, num_heads=8, mlp_ratio=4.0,num_patches=8, qkv_bias=True,\n",
    "                 qk_scale=None, representation_size=None, distilled=False, drop_ratio=0.3,\n",
    "                 attn_drop_ratio=0.3, drop_path_ratio=0.3, embed_layer=PatchEmbed, norm_layer=None,\n",
    "                 act_layer=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_size (int, tuple): input image size\n",
    "            patch_size (int, tuple): patch size\n",
    "            in_c (int): number of input channels\n",
    "            num_classes (int): number of classes for classification head\n",
    "            embed_dim (int): embedding dimension\n",
    "            depth (int): depth of transformer\n",
    "            num_heads (int): number of attention heads\n",
    "            mlp_ratio (int): ratio of mlp hidden dim to embedding dim\n",
    "            qkv_bias (bool): enable bias for qkv if True\n",
    "            qk_scale (float): override default qk scale of head_dim ** -0.5 if set\n",
    "            representation_size (Optional[int]): enable and set representation layer (pre-logits) to this value if set\n",
    "            distilled (bool): model includes a distillation token and head as in DeiT models\n",
    "            drop_ratio (float): dropout rate\n",
    "            attn_drop_ratio (float): attention dropout rate\n",
    "            drop_path_ratio (float): stochastic depth rate\n",
    "            embed_layer (nn.Module): patch embedding layer\n",
    "            norm_layer: (nn.Module): normalization layer\n",
    "        \"\"\"\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n",
    "        self.num_tokens = 2 if distilled else 1\n",
    "        norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)\n",
    "        act_layer = act_layer or nn.GELU\n",
    "\n",
    "        self.patch_embed = embed_layer(embed_dim=embed_dim)\n",
    "        \n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.dist_token = nn.Parameter(torch.zeros(1, 1, embed_dim)) if distilled else None\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + self.num_tokens, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(p=drop_ratio)\n",
    "\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_ratio, depth)]  # stochastic depth decay rule\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                  drop_ratio=drop_ratio, attn_drop_ratio=attn_drop_ratio, drop_path_ratio=dpr[i],\n",
    "                  norm_layer=norm_layer, act_layer=act_layer)\n",
    "            for i in range(depth)\n",
    "        ])\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "\n",
    "        # Representation layer\n",
    "        if representation_size and not distilled:\n",
    "            self.has_logits = True\n",
    "            self.num_features = representation_size\n",
    "            self.pre_logits = nn.Sequential(OrderedDict([\n",
    "                (\"fc\", nn.Linear(embed_dim, representation_size)),\n",
    "                (\"act\", nn.Tanh())\n",
    "            ]))\n",
    "        else:\n",
    "            self.has_logits = False\n",
    "            self.pre_logits = nn.Identity()\n",
    "\n",
    "        # Classifier head(s)\n",
    "        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n",
    "        self.head_dist = None\n",
    "        if distilled:\n",
    "            self.head_dist = nn.Linear(self.embed_dim, self.num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "        # Weight init\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        if self.dist_token is not None:\n",
    "            nn.init.trunc_normal_(self.dist_token, std=0.02)\n",
    "\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "        self.apply(_init_vit_weights)\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        # [B, C, H, W] -> [B, num_patches, embed_dim]\n",
    "        x = self.patch_embed(x)  # [B, 2, 128]\n",
    "        # [1, 1, 128]-> [B, 1, 128]\n",
    "        cls_token = self.cls_token.expand(x.shape[0], -1, -1)\n",
    "        if self.dist_token is None:\n",
    "            x = torch.cat((cls_token, x), dim=1)  # [B, 3, 128]\n",
    "        else:\n",
    "            x = torch.cat((cls_token, self.dist_token.expand(x.shape[0], -1, -1), x), dim=1)\n",
    "\n",
    "        x = self.pos_drop(x + self.pos_embed)\n",
    "        x = self.blocks(x)\n",
    "        x = self.norm(x)\n",
    "        if self.dist_token is None:\n",
    "            return self.pre_logits(x[:, 0])\n",
    "        else:\n",
    "            return x[:, 0], x[:, 1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        if self.head_dist is not None:\n",
    "            x, x_dist = self.head(x[0]), self.head_dist(x[1])\n",
    "            if self.training and not torch.jit.is_scripting():\n",
    "                # during inference, return the average of both classifier predictions\n",
    "                return x, x_dist\n",
    "            else:\n",
    "                return (x + x_dist) / 2\n",
    "        else:\n",
    "            x = self.head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def _init_vit_weights(m):\n",
    "    \"\"\"\n",
    "    ViT weight initialization\n",
    "    :param m: module\n",
    "    \"\"\"\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.trunc_normal_(m.weight, std=.01)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(m.weight, mode=\"fan_out\")\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.LayerNorm):\n",
    "        nn.init.zeros_(m.bias)\n",
    "        nn.init.ones_(m.weight)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fmodel: fusion transformer model\n",
    "# from fusiontransformer_vit import vit_base_patch16_224 as f\n",
    "device = torch.device(\"cuda\")\n",
    "fmodel = VisionTransformer().to(device)\n",
    "\n",
    "# model-1: resnet18\n",
    "model_1 = torch.load('model_1_分层.pth',map_location=device).to(device)\n",
    "# model-2: DNN\n",
    "model_2=Net(6, 512, 7).to(device)\n",
    "model_2.load_state_dict(torch.load('model_2_param.pkl',map_location=device))\n",
    "#model_2 = torch.load('model_2_分层.pth',map_location=device).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complex\n",
    "import complexPyTorch\n",
    "from complexPyTorch.complexLayers import ComplexBatchNorm2d, ComplexConv2d, ComplexLinear\n",
    "from complexPyTorch.complexLayers import ComplexDropout2d, NaiveComplexBatchNorm2d\n",
    "from complexPyTorch.complexLayers import ComplexBatchNorm1d\n",
    "from complexPyTorch.complexFunctions import complex_relu, complex_max_pool2d\n",
    "iscomplex = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features\n",
    "def my_forward(model, x):\n",
    "    mo = nn.Sequential(*list(model.children())[:-1])\n",
    "    feature = mo(x)\n",
    "    feature = feature.view(x.size(0), -1)\n",
    "    output= model.fc(feature)\n",
    "    return feature, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "multi-target not supported at /opt/conda/conda-bld/pytorch_1607370117127/work/aten/src/THCUNN/generic/ClassNLLCriterion.cu:15",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)\n",
      "Input \u001b[0;32mIn [128]\u001b[0m, in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n",
      "\u001b[1;32m     45\u001b[0m out \u001b[38;5;241m=\u001b[39m fmodel(input3)\n",
      "\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(label\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;32m---> 47\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     48\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m*\u001b[39mlabel\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;32m     49\u001b[0m _,pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(out,\u001b[38;5;241m1\u001b[39m)\n",
      "\n",
      "File \u001b[0;32m~/.conda/envs/Henry/lib/python3.8/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    725\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;32m    726\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m--> 727\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n",
      "\u001b[1;32m    729\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n",
      "\u001b[1;32m    730\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n",
      "\u001b[1;32m    731\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "\n",
      "File \u001b[0;32m~/.conda/envs/Henry/lib/python3.8/site-packages/torch/nn/modules/loss.py:961\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n",
      "\u001b[1;32m    960\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n",
      "\u001b[0;32m--> 961\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    962\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/.conda/envs/Henry/lib/python3.8/site-packages/torch/nn/functional.py:2468\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n",
      "\u001b[1;32m   2466\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;32m   2467\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n",
      "\u001b[0;32m-> 2468\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnll_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_softmax\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/.conda/envs/Henry/lib/python3.8/site-packages/torch/nn/functional.py:2264\u001b[0m, in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n",
      "\u001b[1;32m   2261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected input batch_size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) to match target batch_size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[1;32m   2262\u001b[0m                      \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), target\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)))\n",
      "\u001b[1;32m   2263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "\u001b[0;32m-> 2264\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnll_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   2265\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m dim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m:\n",
      "\u001b[1;32m   2266\u001b[0m     ret \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mnll_loss2d(\u001b[38;5;28minput\u001b[39m, target, weight, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction), ignore_index)\n",
      "\n",
      "\u001b[0;31mRuntimeError\u001b[0m: multi-target not supported at /opt/conda/conda-bld/pytorch_1607370117127/work/aten/src/THCUNN/generic/ClassNLLCriterion.cu:15"
     ]
    }
   ],
   "source": [
    "# fmodel training\n",
    "learning_rate = 0.02\n",
    "num_epoches = 20\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)     # 设置学习率下降策略\n",
    "optimizer=optim.SGD(fmodel.parameters(),lr=learning_rate)\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "\n",
    "train_loss_all = []   # store the loss of training set\n",
    "train_accur_all = []  # store the accuracy of training set\n",
    "test_loss_all = []    # store the loss of test set\n",
    "test_accur_all = []   # store the accuracy of test set\n",
    "model_1.eval()\n",
    "model_2.eval()\n",
    "#开始训练\n",
    "for epoch in range(num_epoches):\n",
    "    \n",
    "    running_loss=0.0\n",
    "    running_acc=0.0\n",
    "    fmodel.train()\n",
    "    for i,data in enumerate(train_loader_3,1):\n",
    "        img_plus,label=data\n",
    "        img=img_plus[1]\n",
    "        sub_label=img_plus[0]\n",
    "        img=Variable(img).to(device)\n",
    "        sub_label = sub_label.float()\n",
    "        label=Variable(label).to(device)\n",
    "        sub_label=sub_label.to(device)\n",
    "        #forward\n",
    "        feature_1, out1 = my_forward(model_1,img)\n",
    "        feature_2, out2 = my_forward(model_2,sub_label)\n",
    "        #out1=model_1(img)\n",
    "        #out2=model_2(sub_label)\n",
    "        if iscomplex:\n",
    "            feature_1 = feature_1.unsqueeze(1)\n",
    "            feature_2 = feature_2.unsqueeze(1)\n",
    "            input3 = torch.cat((feature_1,feature_2),1)\n",
    "        else:    \n",
    "            cca = CCA(n_components = 2)\n",
    "            out1 = out1.cpu()\n",
    "            out2 = out2.cpu()\n",
    "            out1 = out1.detach().numpy()\n",
    "            out2 = out2.detach().numpy()\n",
    "            cca.fit(out1, out2)\n",
    "            X_c, Y_c = cca.transform(out1, out2)\n",
    "            input3 = torch.Tensor(np.hstack((X_c,Y_c))).to(device)\n",
    "        out = fmodel(input3)\n",
    "        print(label.shape)\n",
    "        loss = criterion(out,label)\n",
    "        running_loss += loss.data*label.size(0)\n",
    "        _,pred = torch.max(out,1)\n",
    "        num_correct = (pred==label).sum()\n",
    "        running_acc += num_correct.data\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print('Train {} epoch, Loss: {:.6f},Acc: {:.6f}'.format(epoch+1,running_loss/(len(train_data_3)),running_acc/len(train_data_3)))\n",
    "    train_loss_all.append(running_loss / len(train_data_3))   # store the loss of training set, and then plot it\n",
    "    train_accur_all.append(running_acc/len(train_data_3))     # store the accuracy of training set, and then plot it\n",
    "    #scheduler.step() # update the learning rate\n",
    "    \n",
    "    fmodel.eval()\n",
    "    with torch.no_grad():\n",
    "        eval_loss = 0\n",
    "        eval_acc = 0\n",
    "        train_loss_results = []\n",
    "\n",
    "        for data in test_loader_3:\n",
    "            img_plus,label=data\n",
    "            img=img_plus[1]\n",
    "            sub_label=img_plus[0]\n",
    "            img=Variable(img).to(device)\n",
    "            sub_label = sub_label.float()\n",
    "            label=Variable(label).to(device)\n",
    "            sub_label=sub_label.to(device)\n",
    "            # forward\n",
    "            feature_1, out1 = my_forward(model_1,img)\n",
    "            feature_2, out2 = my_forward(model_2,sub_label)\n",
    "            if iscomplex:\n",
    "                feature_1 = feature_1.unsqueeze(1)\n",
    "                feature_2 = feature_2.unsqueeze(1)\n",
    "                input3 = torch.cat((feature_1,feature_2),1)\n",
    "            else:\n",
    "                cca = CCA(n_components = 2)\n",
    "                out1 = out1.cpu()\n",
    "                out2 = out2.cpu()\n",
    "                out1 = out1.detach().numpy()\n",
    "                out2 = out2.detach().numpy()\n",
    "                cca.fit(out1, out2)\n",
    "                X_c, Y_c = cca.transform(out1, out2)\n",
    "                input3 = torch.Tensor(np.hstack((X_c,Y_c))).to(device)\n",
    "            out = fmodel(input3)\n",
    "            loss = criterion(out, label)\n",
    "            eval_loss += loss.data\n",
    "            _,pred = torch.max(out,1)\n",
    "            num_correct = (pred == label).sum()\n",
    "            eval_acc += num_correct.data\n",
    "\n",
    "    print('Test Loss: {:,.6f}, Acc: {:,.6f}'.format(eval_loss/(len(test_data_1)), eval_acc/(len(test_data_1))))\n",
    "    test_loss_all.append(eval_loss/(len(test_data_1)))\n",
    "    test_accur_all.append(eval_acc/(len(test_data_1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x288 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 数据准备，转移到CPU\n",
    "for i in range(len(train_loss_all)):\n",
    "    train_loss_all[i] = torch.Tensor.cpu(train_loss_all[i])\n",
    "    train_accur_all[i] = torch.Tensor.cpu(train_accur_all[i])\n",
    "    test_loss_all[i] = torch.Tensor.cpu(test_loss_all[i])\n",
    "    test_accur_all[i] = torch.Tensor.cpu(test_accur_all[i])\n",
    "\n",
    "epoches = [i+1 for i in range(num_epoches)]\n",
    "\n",
    "# 画图可视化\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(epoches, train_loss_all,\"ro-\", label=\"Train loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(epoches, train_accur_all,\"ro-\", label=\"Train accur\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"acc\")\n",
    "plt.legend()\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(epoches, test_loss_all,\"bo-\", label=\"Test loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(epoches, test_accur_all,\"bo-\", label=\"Test accur\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"acc\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save fmodel\n",
    "torch.save(fmodel, 'fmodel_name.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
