{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33a5460c",
   "metadata": {},
   "source": [
    "#### Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a766c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn,optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "import torchvision.models as models\n",
    "from torch.optim import SGD\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Hyperparameters\n",
    "torch.manual_seed(1)\n",
    "batch_size= 60\n",
    "learning_rate=0.002\n",
    "num_epoches=15\n",
    "num_classes=7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7977cd8e",
   "metadata": {},
   "source": [
    "#### Read data from images and excel-labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6b2e939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the images and names\n",
    "IMAGE_DIR = 'C_img_jpg'\n",
    "def read_images(image_path=IMAGE_DIR):\n",
    "    images = []\n",
    "    images_names = [image for image in os.listdir(image_path) if not image.startswith('.')] \n",
    "    for image_name in images_names: \n",
    "            img = Image.open (os.path.join(image_path, image_name))\n",
    "            if Random:\n",
    "                img_transforms = transforms.Compose([transforms.Resize(256),\n",
    "                                       transforms.CenterCrop(224),\n",
    "                                       transforms.RandomHorizontalFlip(0.5),\n",
    "                                       transforms.ToTensor()\n",
    "                                       ])\n",
    "                tensor = img_transforms(img)\n",
    "                images.append(tensor)\n",
    "            else:\n",
    "                images.append(img)\n",
    "    return images,images_names\n",
    "images,names = read_images()\n",
    "\n",
    "# read the label\n",
    "excel=pd.read_csv('C_all_data_add_20211007_xz.csv', encoding = 'gb2312')\n",
    "excel = np.array(excel)\n",
    "excel = excel.tolist()\n",
    "# Alligning the labels with images. (i.e. the first label corresponds to the first label.)\n",
    "labels = [] \n",
    "sub_labels = [] \n",
    "for i in range(len(names)):\n",
    "    for j in range(len(excel)):\n",
    "        if names[i] == excel[j][0]:\n",
    "            labels.append(excel[j][1])\n",
    "            sub_labels.append(excel[j][2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ea2078",
   "metadata": {},
   "source": [
    "#### Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15807fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Random = 1\n",
    "if Random:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    final_input=[]\n",
    "    for i in range(len(names)):\n",
    "        final_input.append((torch.Tensor(sub_labels[i]),images[i]))\n",
    "    \n",
    "    X_train_3,X_test_3,y_train_3,y_test_3=train_test_split(final_input,labels,test_size=0.2,random_state=42)\n",
    "    train_data_3=[]\n",
    "    test_data_3=[]\n",
    "    for i in range(len(X_train_3)):\n",
    "        train_data_3.append((X_train_3[i],y_train_3[i]))\n",
    "    for i in range(len(X_test_3)):\n",
    "        test_data_3.append((X_test_3[i],y_test_3[i]))\n",
    "\n",
    "else:\n",
    "    # Stratified Random Sampling\n",
    "    from sklearn.model_selection import StratifiedShuffleSplit\n",
    "    split = StratifiedShuffleSplit(n_splits = 1,test_size = 0.2,random_state = 42)\n",
    "\n",
    "    names = np.array(names)\n",
    "    labels = np.array(labels)\n",
    "    names = names.reshape((names.shape[0],-1))\n",
    "    labels = labels.reshape((labels.shape[0],-1))\n",
    "    data = np.hstack((names,labels)) # hstack:each name corresponds to one label \n",
    "    for train_index,test_index in split.split(data,data[:,-1]):\n",
    "        train_set = data[train_index,:]\n",
    "        test_set = data[test_index,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c565f45f",
   "metadata": {},
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4351ec5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data transformation\n",
    "train_transforms = transforms.Compose([transforms.RandomHorizontalFlip(0.5),\n",
    "                                        transforms.Resize(256),\n",
    "                                        transforms.CenterCrop(224),\n",
    "                                        transforms.ToTensor()\n",
    "                                        ])\n",
    "\n",
    "train_data_1 = []\n",
    "train_data_3 = []\n",
    "for row in range(len(train_set)):\n",
    "    for i in range(len(names)):\n",
    "        if names[i] == train_set[row][0]: \n",
    "            img_data = train_transforms(images[i]) \n",
    "            # train_set is [names, labels], we need to get the train_data_1 [images, labels], train_data_3 [images, sub_labels, labels]\n",
    "            train_data_1.append((img_data,int(train_set[row][1]))) \n",
    "            train_data_3.append((img_data,torch.Tensor(sub_labels[i][:]),int(train_set[row][1])))\n",
    "\n",
    "test_transforms =  transforms.Compose([transforms.Resize(256),\n",
    "                                       transforms.CenterCrop(224),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       \n",
    "                                       ])\n",
    "test_data_1 = []\n",
    "test_data_3 = []\n",
    "for row in range(len(test_set)):\n",
    "    for i in range(len(names)):\n",
    "        if names[i] == test_set[row][0]:\n",
    "            img_data = test_transforms(images[i])\n",
    "            test_data_1.append((img_data,int(test_set[row][1]))) \n",
    "            test_data_3.append((img_data,torch.Tensor(sub_labels[i][:]),int(train_set[row][1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d927c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader\n",
    "test_size = 0.2\n",
    "# sdudent_train_loader = DataLoader(student_train , batch_size = batch_size ,shuffle=True)\n",
    "# student_test_loader = DataLoader(student_test, batch_size = batch_size, shuffle=False) #int(batch_size*test_size/(1-test_size))\n",
    "teacher_train_loader = DataLoader(train_data_3 , batch_size=batch_size ,shuffle=True)\n",
    "teacher_test_loader = DataLoader(test_data_3, batch_size=batch_size,shuffle=False )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf626ae",
   "metadata": {},
   "source": [
    "#### Load the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b80e3bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "original code from rwightman:\n",
    "https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n",
    "\"\"\"\n",
    "from functools import partial\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
    "    \"\"\"\n",
    "    Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n",
    "    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n",
    "    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n",
    "    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n",
    "    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n",
    "    'survival rate' as the argument.\n",
    "    \"\"\"\n",
    "    if drop_prob == 0. or not training:\n",
    "        return x\n",
    "    keep_prob = 1 - drop_prob\n",
    "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
    "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "    random_tensor.floor_()  # binarize\n",
    "    output = x.div(keep_prob) * random_tensor\n",
    "    return output\n",
    "\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"\n",
    "    Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
    "    \"\"\"\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training)\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\"\n",
    "    2D Image to Patch Embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim=128, norm_layer=None):\n",
    "        super().__init__()\n",
    "        '''img_size = (img_size, img_size)\n",
    "        patch_size = (patch_size, patch_size)\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.grid_size = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])\n",
    "        self.num_patches = self.grid_size[0] * self.grid_size[1]\n",
    "\n",
    "        self.proj = nn.Conv2d(in_c, embed_dim, kernel_size=patch_size, stride=patch_size)'''\n",
    "        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''B, C, H, W = x.shape\n",
    "        assert H == self.img_size[0] and W == self.img_size[1], \\\n",
    "            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "\n",
    "        # flatten: [B, C, H, W] -> [B, C, HW]\n",
    "        # transpose: [B, C, HW] -> [B, HW, C'''\n",
    "        #x = x.transpose(1, 2)  #[B,2,128]\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self,\n",
    "                 dim,   # 输入token的dim\n",
    "                 num_heads=8,\n",
    "                 qkv_bias=False,\n",
    "                 qk_scale=None,\n",
    "                 attn_drop_ratio=0.1,\n",
    "                 proj_drop_ratio=0.1):\n",
    "        super(Attention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop_ratio)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop_ratio)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # [batch_size, num_patches + 1, total_embed_dim] [B, 3, 128]\n",
    "        B, N, C = x.shape\n",
    "\n",
    "        # qkv(): -> [batch_size, num_patches + 1, 3 * total_embed_dim][B, 3, 3*128]\n",
    "        # reshape: -> [batch_size, num_patches + 1, 3, num_heads, embed_dim_per_head]\n",
    "        # permute: -> [3, batch_size, num_heads /8, num_patches + 1 /3, embed_dim_per_head /16]\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        # [batch_size, num_heads, num_patches + 1, embed_dim_per_head] [B,8,3,16]\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        # transpose: -> [batch_size, num_heads, embed_dim_per_head, num_patches + 1]\n",
    "        # @: multiply -> [batch_size, num_heads, num_patches + 1, num_patches + 1] [B,8,3,3]\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        # @: multiply -> [batch_size, num_heads, num_patches + 1, embed_dim_per_head]\n",
    "        # transpose: -> [batch_size, num_patches + 1, num_heads, embed_dim_per_head]\n",
    "        # reshape: -> [batch_size, num_patches + 1, total_embed_dim]\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    \"\"\"\n",
    "    MLP as used in Vision Transformer, MLP-Mixer and related networks\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self,\n",
    "                 dim,\n",
    "                 num_heads,\n",
    "                 mlp_ratio=4.,\n",
    "                 qkv_bias=False,\n",
    "                 qk_scale=None,\n",
    "                 drop_ratio=0.,\n",
    "                 attn_drop_ratio=0.,\n",
    "                 drop_path_ratio=0.,\n",
    "                 act_layer=nn.GELU,\n",
    "                 norm_layer=nn.LayerNorm):\n",
    "        super(Block, self).__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                              attn_drop_ratio=attn_drop_ratio, proj_drop_ratio=drop_ratio)\n",
    "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
    "        self.drop_path = DropPath(drop_path_ratio) if drop_path_ratio > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop_ratio)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.drop_path(self.attn(self.norm1(x)))\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, num_classes=7, embed_dim=128, depth=12, num_heads=8, mlp_ratio=4.0,num_patches=8, qkv_bias=True,\n",
    "                 qk_scale=None, representation_size=None, distilled=False, drop_ratio=0.3,\n",
    "                 attn_drop_ratio=0.3, drop_path_ratio=0.3, embed_layer=PatchEmbed, norm_layer=None,\n",
    "                 act_layer=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_size (int, tuple): input image size\n",
    "            patch_size (int, tuple): patch size\n",
    "            in_c (int): number of input channels\n",
    "            num_classes (int): number of classes for classification head\n",
    "            embed_dim (int): embedding dimension\n",
    "            depth (int): depth of transformer\n",
    "            num_heads (int): number of attention heads\n",
    "            mlp_ratio (int): ratio of mlp hidden dim to embedding dim\n",
    "            qkv_bias (bool): enable bias for qkv if True\n",
    "            qk_scale (float): override default qk scale of head_dim ** -0.5 if set\n",
    "            representation_size (Optional[int]): enable and set representation layer (pre-logits) to this value if set\n",
    "            distilled (bool): model includes a distillation token and head as in DeiT models\n",
    "            drop_ratio (float): dropout rate\n",
    "            attn_drop_ratio (float): attention dropout rate\n",
    "            drop_path_ratio (float): stochastic depth rate\n",
    "            embed_layer (nn.Module): patch embedding layer\n",
    "            norm_layer: (nn.Module): normalization layer\n",
    "        \"\"\"\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n",
    "        self.num_tokens = 2 if distilled else 1\n",
    "        norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)\n",
    "        act_layer = act_layer or nn.GELU\n",
    "\n",
    "        self.patch_embed = embed_layer(embed_dim=embed_dim)\n",
    "        \n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.dist_token = nn.Parameter(torch.zeros(1, 1, embed_dim)) if distilled else None\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + self.num_tokens, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(p=drop_ratio)\n",
    "\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_ratio, depth)]  # stochastic depth decay rule\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                  drop_ratio=drop_ratio, attn_drop_ratio=attn_drop_ratio, drop_path_ratio=dpr[i],\n",
    "                  norm_layer=norm_layer, act_layer=act_layer)\n",
    "            for i in range(depth)\n",
    "        ])\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "\n",
    "        # Representation layer\n",
    "        if representation_size and not distilled:\n",
    "            self.has_logits = True\n",
    "            self.num_features = representation_size\n",
    "            self.pre_logits = nn.Sequential(OrderedDict([\n",
    "                (\"fc\", nn.Linear(embed_dim, representation_size)),\n",
    "                (\"act\", nn.Tanh())\n",
    "            ]))\n",
    "        else:\n",
    "            self.has_logits = False\n",
    "            self.pre_logits = nn.Identity()\n",
    "\n",
    "        # Classifier head(s)\n",
    "        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n",
    "        self.head_dist = None\n",
    "        if distilled:\n",
    "            self.head_dist = nn.Linear(self.embed_dim, self.num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "        # Weight init\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        if self.dist_token is not None:\n",
    "            nn.init.trunc_normal_(self.dist_token, std=0.02)\n",
    "\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "        self.apply(_init_vit_weights)\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        # [B, C, H, W] -> [B, num_patches, embed_dim]\n",
    "        x = self.patch_embed(x)  # [B, 2, 128]\n",
    "        # [1, 1, 128]-> [B, 1, 128]\n",
    "        cls_token = self.cls_token.expand(x.shape[0], -1, -1)\n",
    "        if self.dist_token is None:\n",
    "            x = torch.cat((cls_token, x), dim=1)  # [B, 3, 128]\n",
    "        else:\n",
    "            x = torch.cat((cls_token, self.dist_token.expand(x.shape[0], -1, -1), x), dim=1)\n",
    "\n",
    "        x = self.pos_drop(x + self.pos_embed)\n",
    "        x = self.blocks(x)\n",
    "        x = self.norm(x)\n",
    "        if self.dist_token is None:\n",
    "            return self.pre_logits(x[:, 0])\n",
    "        else:\n",
    "            return x[:, 0], x[:, 1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        if self.head_dist is not None:\n",
    "            x, x_dist = self.head(x[0]), self.head_dist(x[1])\n",
    "            if self.training and not torch.jit.is_scripting():\n",
    "                # during inference, return the average of both classifier predictions\n",
    "                return x, x_dist\n",
    "            else:\n",
    "                return (x + x_dist) / 2\n",
    "        else:\n",
    "            x = self.head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def _init_vit_weights(m):\n",
    "    \"\"\"\n",
    "    ViT weight initialization\n",
    "    :param m: module\n",
    "    \"\"\"\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.trunc_normal_(m.weight, std=.01)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(m.weight, mode=\"fan_out\")\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.LayerNorm):\n",
    "        nn.init.zeros_(m.bias)\n",
    "        nn.init.ones_(m.weight)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1fbdf6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_forward(model, x):\n",
    "    mo = nn.Sequential(*list(model.children())[:-1])\n",
    "    feature = mo(x)\n",
    "    feature = feature.view(x.size(0), -1)\n",
    "    output= model.fc(feature)\n",
    "    return feature, output\n",
    "\n",
    "if Random:\n",
    "    class Net(nn.Module):\n",
    "        def __init__(self, input_size, hidden_size, num_classes=7):\n",
    "            super(Net, self).__init__()\n",
    "            self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "            self.relu = nn.ReLU()\n",
    "            self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "        def forward(self, x):\n",
    "            out= self.fc1(x)\n",
    "            out= self.relu(out)\n",
    "            self.feature=out\n",
    "            out= self.fc(out)\n",
    "            return out\n",
    "\n",
    "else:\n",
    "    class Net(nn.Module):\n",
    "        def __init__(self, input_size, hidden_size, num_classes=7):\n",
    "            super(Net, self).__init__()\n",
    "            self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "            self.relu = nn.ReLU()\n",
    "            self.dropout = nn.Dropout(p=0.5)\n",
    "            self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "            self. fc = nn.Linear(hidden_size, 7)\n",
    "\n",
    "\n",
    "        def forward(self, x):\n",
    "            out= self.fc1(x)\n",
    "            out= self.relu(out)\n",
    "            out= self.dropout(out)\n",
    "            out= self.fc2(out)\n",
    "            out= self.relu(out)\n",
    "            out= self.fc(out)\n",
    "            return out\n",
    "\n",
    "device=torch.device('cuda')\n",
    "torch.cuda.set_device(7)\n",
    "model_2=Net(6, 512, 7).to(device)\n",
    "\n",
    "if Random:\n",
    "    model_1 = torch.load('model_1_Random.pth').to(device)\n",
    "    model_2.load_state_dict(torch.load('model_2_param_Random.pkl'))\n",
    "else:\n",
    "    model_1 = torch.load('model_1_分层.pth').to(device)\n",
    "    model_2.load_state_dict(torch.load('model_2_param.pkl'))\n",
    "\n",
    "per_patch = 1\n",
    "# Teacher model\n",
    "fmodel = torch.load('fmodel_with 1 token.pth').to(device)\n",
    "\n",
    "# Student model\n",
    "student = models.resnet18(pretrained = True)   \n",
    "student.fc = nn.Sequential(\n",
    "    nn.Linear(student.fc.in_features, 7)\n",
    ")\n",
    "student = student.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15be1076",
   "metadata": {},
   "source": [
    "#### Train the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "21cbf7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 1 epoch, Loss: 0.020741,Acc: 0.985465\n",
      "Test Loss: 0.013985, Acc: 0.776744\n",
      "Train 2 epoch, Loss: 0.017469,Acc: 0.991279\n",
      "Test Loss: 0.013172, Acc: 0.765116\n",
      "Train 3 epoch, Loss: 0.016094,Acc: 0.993605\n",
      "Test Loss: 0.012403, Acc: 0.776744\n",
      "Train 4 epoch, Loss: 0.016114,Acc: 0.993023\n",
      "Test Loss: 0.013892, Acc: 0.755814\n",
      "Train 5 epoch, Loss: 0.015421,Acc: 0.995349\n",
      "Test Loss: 0.012216, Acc: 0.790698\n",
      "Train 6 epoch, Loss: 0.016023,Acc: 0.991860\n",
      "Test Loss: 0.013071, Acc: 0.788372\n",
      "Train 7 epoch, Loss: 0.013930,Acc: 0.995930\n",
      "Test Loss: 0.012938, Acc: 0.783721\n",
      "Train 8 epoch, Loss: 0.014895,Acc: 0.991860\n",
      "Test Loss: 0.012438, Acc: 0.795349\n",
      "Train 9 epoch, Loss: 0.013312,Acc: 0.995349\n",
      "Test Loss: 0.014450, Acc: 0.751163\n",
      "Train 10 epoch, Loss: 0.015565,Acc: 0.993023\n",
      "Test Loss: 0.013276, Acc: 0.755814\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "num_epoches = 10\n",
    "# distillation temperature\n",
    "temp = 6\n",
    "# hard_loss\n",
    "hard_loss = nn.CrossEntropyLoss()\n",
    "# hard_loss weight\n",
    "alpha = 0.1\n",
    "# soft_loss\n",
    "soft_loss = nn.KLDivLoss(reduction='batchmean') # KL divergence loss\n",
    "#optimizer = optim.Adam(student.parameters(),lr = learning_rate)\n",
    "optimizer = optim.SGD(student.parameters(),lr=learning_rate)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)     # learning rate decay\n",
    "per_patch = 1\n",
    "\n",
    "model_1.eval()\n",
    "model_2.eval()\n",
    "fmodel.eval()\n",
    "student.train()\n",
    "\n",
    "train_loss_all = []   # store the loss of training set\n",
    "train_accur_all = []  # store the accuracy of training set\n",
    "test_loss_all = []    # store the loss of test set\n",
    "test_accur_all = []   # store the accuracy of test set\n",
    "\n",
    "# train the student model\n",
    "for epoch in range(num_epoches):\n",
    "    running_loss=0.0\n",
    "    running_acc=0.0\n",
    "    student.train()\n",
    "    for j,data in enumerate(teacher_train_loader,1):\n",
    "        img_plus, label = data\n",
    "        img = img_plus[1]\n",
    "        sub_label = img_plus[0]\n",
    "        img = Variable(img).to(device)\n",
    "        sub_label = Variable(sub_label).to(device)\n",
    "        label = Variable(label).to(device) \n",
    "        \n",
    "        # calculate the teacher model prediction\n",
    "        with torch.no_grad():\n",
    "            model_1, model_2, fmodel = model_1.to(device), model_2.to(device), fmodel.to(device)\n",
    "            feature1, out1 = my_forward(model_1,img)\n",
    "            feature2, out2 = my_forward(model_2,sub_label)\n",
    "            '''for i in range(per_patch):\n",
    "                exec(\"img_token_%i = feature1[:,128*i:128*(i+1)].unsqueeze(1)\"%i)\n",
    "                exec(\"label_token_%i = feature2[:,128*i:128*(i+1)].unsqueeze(1)\"%i)\n",
    "            input3 = torch.cat((img_token_0,img_token_1,img_token_2,img_token_3),1)\n",
    "            input3 = torch.cat((input3,label_token_0,label_token_1,label_token_2,label_token_3),1)'''\n",
    "            feature1 = feature1.unsqueeze(1)\n",
    "            feature2 = feature2.unsqueeze(1)\n",
    "            input3 = torch.cat((feature1,feature2),1)\n",
    "            teacher_preds = fmodel(input3)\n",
    "\n",
    "        # calculate the student model prediction\n",
    "        student_preds = student(img)\n",
    "        \n",
    "        # calculate the hard_loss\n",
    "        student_loss = hard_loss(student_preds, label)\n",
    "        # calculate the soft_loss\n",
    "        distillation_loss = soft_loss(\n",
    "                            F.log_softmax(student_preds/temp, dim=1),\n",
    "                            F.softmax(teacher_preds/temp, dim=1)\n",
    "        )\n",
    "        \n",
    "        # calculate the total loss by weighted sum of hard_loss and soft_loss\n",
    "        loss = alpha * student_loss + (1 - alpha) * distillation_loss\n",
    "        \n",
    "        running_loss += loss.data * label.size(0)\n",
    "        _, pred = torch.max(student_preds,1)\n",
    "        num_correct = (pred==label).sum()\n",
    "        running_acc += num_correct.data\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #scheduler.step() # update learning rate\n",
    "    \n",
    "    print('Train {} epoch, Loss: {:.6f},Acc: {:.6f}'.format(epoch+1,running_loss/(len(train_data_3)),running_acc/len(train_data_3)))\n",
    "    train_loss_all.append(running_loss / len(train_data_3))   # store the loss of training set, and then plot it\n",
    "    train_accur_all.append(running_acc/len(train_data_3))     # store the accuracy of training set, and then plot it\n",
    "    \n",
    "    # evaluate the student model\n",
    "    student.eval()\n",
    "    with torch.no_grad():\n",
    "        eval_loss = 0\n",
    "        eval_acc = 0\n",
    "        train_loss_results = []\n",
    "        \n",
    "        for data in teacher_test_loader:\n",
    "            img_plus, label = data\n",
    "            img = img_plus[1]\n",
    "            sub_label = img_plus[0]\n",
    "            img = Variable(img).to(device)\n",
    "            sub_label = Variable(sub_label).to(device)\n",
    "            label = Variable(label).to(device) \n",
    "            \n",
    "            student_preds = student(img)\n",
    "            loss = hard_loss(student_preds, label)\n",
    "            eval_loss += loss.data\n",
    "            _,pred = torch.max(student_preds,1)\n",
    "            num_correct = (pred == label).sum()\n",
    "            eval_acc += num_correct.data\n",
    "\n",
    "    print('Test Loss: {:,.6f}, Acc: {:,.6f}'.format(eval_loss/(len(test_data_3)), eval_acc/(len(test_data_3))))\n",
    "    test_loss_all.append(eval_loss/(len(test_data_3)))\n",
    "    test_accur_all.append(eval_acc/(len(test_data_3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde5b415",
   "metadata": {},
   "source": [
    "#### Save the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37c24199",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(student,'student_new.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91bc45a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf76889",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
