{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn,optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "import torchvision.models as models\n",
    "from torch.optim import SGD\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# parameters\n",
    "torch.manual_seed(1)\n",
    "batch_size = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read data from images and excel-labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the images and names\n",
    "IMAGE_DIR =  'Data/C_img_jpg'\n",
    "def read_images(image_path=IMAGE_DIR):\n",
    "    images = []\n",
    "    images_names = [image for image in os.listdir(image_path) if not image.startswith('.')] \n",
    "    for image_name in images_names: \n",
    "            img = Image.open (os.path.join(image_path, image_name))\n",
    "            images.append(img)\n",
    "    return images,images_names\n",
    "images,names = read_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the label\n",
    "excel=pd.read_csv('Data/C_all_data_add_20211007_xz.csv', encoding = 'gb2312')\n",
    "excel = np.array(excel)\n",
    "excel = excel.tolist()\n",
    "# Alligning the labels with images. (i.e. the first label corresponds to the first label.)\n",
    "labels = []\n",
    "sub_labels = []\n",
    "\n",
    "for i in range(len(names)):\n",
    "    for j in range(len(excel)):\n",
    "        if names[i] == excel[j][0]:\n",
    "            labels.append(excel[j][1])\n",
    "            sub_labels.append(excel[j][2:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the training and testing dataet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified Random Sampling\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "split = StratifiedShuffleSplit(n_splits = 1,test_size = 0.2,random_state = 42)\n",
    "\n",
    "names = np.array(names)\n",
    "labels = np.array(labels)\n",
    "names = names.reshape((names.shape[0],-1))\n",
    "labels = labels.reshape((labels.shape[0],-1))\n",
    "data = np.hstack((names,labels)) # hstack:each name corresponds to one label \n",
    "for train_index,test_index in split.split(data,data[:,-1]):\n",
    "    train_set = data[train_index,:]\n",
    "    test_set = data[test_index,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data trsnformation\n",
    "train_transforms = transforms.Compose([transforms.RandomHorizontalFlip(0.5),\n",
    "                                       transforms.Resize(256),\n",
    "                                       transforms.CenterCrop(224),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       \n",
    "                                       ])\n",
    "train_data_1 = []\n",
    "train_data_3 = []\n",
    "for row in range(len(train_set)):\n",
    "    for i in range(len(names)):\n",
    "        if names[i] == train_set[row][0]: \n",
    "            img_data = train_transforms(images[i]) \n",
    "            # train_set is [names, labels], we need to get the train_data_1 [images, labels], train_data_3 [images, sub_labels, labels]\n",
    "            train_data_1.append((img_data,int(train_set[row][1]))) \n",
    "            train_data_3.append((img_data,torch.Tensor(sub_labels[i][:]),int(train_set[row][1])))\n",
    "\n",
    "test_transforms =  transforms.Compose([transforms.Resize(256),\n",
    "                                       transforms.CenterCrop(224),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       \n",
    "                                       ])\n",
    "test_data_1 = []\n",
    "test_data_3 = []\n",
    "for row in range(len(test_set)):\n",
    "    for i in range(len(names)):\n",
    "        if names[i] == test_set[row][0]:\n",
    "            img_data = test_transforms(images[i])\n",
    "            test_data_1.append((img_data,int(test_set[row][1]))) \n",
    "            test_data_3.append((img_data,torch.Tensor(sub_labels[i][:]),int(train_set[row][1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "test_size = 0.2\n",
    "train_loader_1 = DataLoader(train_data_1 , batch_size = batch_size ,shuffle=True)\n",
    "test_loader_1 = DataLoader(test_data_1, batch_size = int(batch_size*test_size/(1-test_size)), shuffle=False) #int(batch_size*test_size/(1-test_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model-1: resnet18 \n",
    "device=torch.device('cuda')\n",
    "model_1 = models.resnet18(pretrained=True)\n",
    "\n",
    "# modify the last layer\n",
    "model_1.fc =  nn.Linear(model_1.fc.in_features, 7)\n",
    "model_1 = model_1.to(device)\n",
    "print(model_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mixup_data\n",
    "def mixup_data(x, y, alpha=1.0, use_cuda=True):\n",
    "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "    batch_size = x.size()[0]\n",
    "    if use_cuda:\n",
    "        index = torch.randperm(batch_size).cuda()\n",
    "    else:\n",
    "        index = torch.randperm(batch_size)\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 1 training\n",
    "learning_rate = 0.005\n",
    "num_epoches = 20\n",
    "num_classes = 7\n",
    "#alpha = 0.2\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "optimizer=optim.SGD(model_1.parameters(),lr=learning_rate)\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)     # learning rate decay\n",
    "\n",
    "train_loss_all = []   # store the loss of training set\n",
    "train_accur_all = []  # store the accuracy of training set\n",
    "test_loss_all = []    # store the loss of test set\n",
    "test_accur_all = []   # store the accuracy of test set\n",
    "\n",
    "# start training\n",
    "for epoch in range(num_epoches):\n",
    "    running_loss=0.0\n",
    "    running_acc=0.0\n",
    "    model_1.train()\n",
    "    for i,data in enumerate(train_loader_1,1):\n",
    "        img,label=data\n",
    "        img=img.cuda()\n",
    "        label=label.cuda()\n",
    "        #data, labels_a, labels_b, lam = mixup_data(img, label, alpha)\n",
    "        img=Variable(img)\n",
    "        label=Variable(label)\n",
    "        \n",
    "        # forward\n",
    "        out=model_1(img)\n",
    "        loss=criterion(out,label)\n",
    "        #loss = mixup_criterion(criterion, out, labels_a, labels_b, lam)\n",
    "        running_loss+=loss.data*label.size(0)\n",
    "        _,pred=torch.max(out,1)\n",
    "        num_correct=(pred==label).sum()\n",
    "        running_acc+=num_correct.data\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #scheduler.step() # update learning rate\n",
    "    \n",
    "    print('Train {} epoch, Loss: {:.6f},Acc: {:.6f}'.format(epoch+1,running_loss/(len(train_data_1)),running_acc/len(train_data_1)))\n",
    "    train_loss_all.append(running_loss / len(train_data_1))   # store the loss of training set, and then plot it\n",
    "    train_accur_all.append(running_acc/len(train_data_1))     # store the accuracy of training set, and then plot it\n",
    "    \n",
    "    model_1.eval()\n",
    "    with torch.no_grad():\n",
    "        eval_loss = 0\n",
    "        eval_acc = 0\n",
    "        train_loss_results = []\n",
    "\n",
    "        for data in test_loader_1:\n",
    "            img, label = data\n",
    "            img = Variable(img)\n",
    "            img=img.cuda()\n",
    "            label=label.cuda()\n",
    "            out = model_1(img)\n",
    "            loss = criterion(out, label)\n",
    "            eval_loss += loss.data\n",
    "            _,pred = torch.max(out,1)\n",
    "            num_correct = (pred == label).sum()\n",
    "            eval_acc += num_correct.data\n",
    "\n",
    "    print('Test Loss: {:,.6f}, Acc: {:,.6f}'.format(eval_loss/(len(test_data_1)), eval_acc/(len(test_data_1))))\n",
    "    test_loss_all.append(eval_loss/(len(test_data_1)))\n",
    "    test_accur_all.append(eval_acc/(len(test_data_1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer the data to cpu\n",
    "for i in range(len(train_loss_all)):\n",
    "    train_loss_all[i] = torch.Tensor.cpu(train_loss_all[i])\n",
    "    train_accur_all[i] = torch.Tensor.cpu(train_accur_all[i])\n",
    "    test_loss_all[i] = torch.Tensor.cpu(test_loss_all[i])\n",
    "    test_accur_all[i] = torch.Tensor.cpu(test_accur_all[i])\n",
    "\n",
    "epoches = [i+1 for i in range(num_epoches)]\n",
    "\n",
    "# plot the loss and accuracy\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(epoches, train_loss_all,\"ro-\", label=\"Train loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(epoches, train_accur_all,\"ro-\", label=\"Train accur\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"acc\")\n",
    "plt.legend()\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(epoches, test_loss_all,\"bo-\", label=\"Test loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(epoches, test_accur_all,\"bo-\", label=\"Test accur\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"acc\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(model_1, 'model_1_new.pth')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
